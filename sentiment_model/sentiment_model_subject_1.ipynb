{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLzS9tm1lrVK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from PIL import Image\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import ImageFolder\n",
    "import zipfile\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from rich.console import Console\n",
    "from rich.progress import Progress, BarColumn, TimeElapsedColumn, TimeRemainingColumn\n",
    "from rich.table import Table\n",
    "\n",
    "import pandas as pd\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, multilabel_confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bmK9zElXl2Sj",
    "outputId": "2aae60f6-39f8-4647-c38f-8ee70e3a4a8b"
   },
   "outputs": [],
   "source": [
    "# Install Git in case it's not available in your environment\n",
    "!apt-get install git\n",
    "\n",
    "# Clone the repository from GitHub\n",
    "!git clone https://github.com/Samin1362/CSE499-MerakiNexus-AI_And_Defi.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "xYQF3ZWRl56T",
    "outputId": "9a9e34f1-4dfd-4c17-b528-d7dcbe15b99c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets\n",
    "\n",
    "# Define dataset path\n",
    "dataset_path = \"/content/CSE499-MerakiNexus-AI_And_Defi/sentiment_model/sentiment_dataset\"  # Modify with your dataset path\n",
    "\n",
    "# Load the datasets\n",
    "train_dataset = datasets.ImageFolder(root=f\"{dataset_path}/train\")\n",
    "val_dataset = datasets.ImageFolder(root=f\"{dataset_path}/val\")\n",
    "test_dataset = datasets.ImageFolder(root=f\"{dataset_path}/test\")\n",
    "\n",
    "# Get class names and their corresponding folder paths\n",
    "class_names = train_dataset.classes\n",
    "train_image_counts = []\n",
    "val_image_counts = []\n",
    "test_image_counts = []\n",
    "\n",
    "# Count the number of images in each class folder for all datasets\n",
    "for class_name in class_names:\n",
    "    # Count for training set\n",
    "    train_class_folder = os.path.join(train_dataset.root, class_name)\n",
    "    train_image_count = len(os.listdir(train_class_folder))  # Count files in each class folder\n",
    "    train_image_counts.append(train_image_count)\n",
    "\n",
    "    # Count for validation set\n",
    "    val_class_folder = os.path.join(val_dataset.root, class_name)\n",
    "    val_image_count = len(os.listdir(val_class_folder))  # Count files in each class folder\n",
    "    val_image_counts.append(val_image_count)\n",
    "\n",
    "    # Count for testing set\n",
    "    test_class_folder = os.path.join(test_dataset.root, class_name)\n",
    "    test_image_count = len(os.listdir(test_class_folder))  # Count files in each class folder\n",
    "    test_image_counts.append(test_image_count)\n",
    "\n",
    "# Plotting the class distribution for training, validation, and testing sets side by side\n",
    "fig, axes = plt.subplots(1, 4, figsize=(24, 6))\n",
    "\n",
    "# Plot for training dataset\n",
    "axes[0].bar(class_names, train_image_counts, color='skyblue')\n",
    "axes[0].set_title('Training Image Count per Class', fontsize=16)\n",
    "axes[0].set_xlabel('Class', fontsize=14)\n",
    "axes[0].set_ylabel('Number of Images', fontsize=14)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot for validation dataset\n",
    "axes[1].bar(class_names, val_image_counts, color='lightgreen')\n",
    "axes[1].set_title('Validation Image Count per Class', fontsize=16)\n",
    "axes[1].set_xlabel('Class', fontsize=14)\n",
    "axes[1].set_ylabel('Number of Images', fontsize=14)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot for testing dataset\n",
    "axes[2].bar(class_names, test_image_counts, color='lightcoral')\n",
    "axes[2].set_title('Testing Image Count per Class', fontsize=16)\n",
    "axes[2].set_xlabel('Class', fontsize=14)\n",
    "axes[2].set_ylabel('Number of Images', fontsize=14)\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "axes[2].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot for combined view (Training, Validation, and Testing)\n",
    "axes[3].bar(class_names, train_image_counts, color='skyblue', label='Train')\n",
    "axes[3].bar(class_names, val_image_counts, color='lightgreen', label='Validation', alpha=0.7)\n",
    "axes[3].bar(class_names, test_image_counts, color='lightcoral', label='Test', alpha=0.7)\n",
    "axes[3].set_title('Combined Image Count per Class (Train, Validation, Test)', fontsize=16)\n",
    "axes[3].set_xlabel('Class', fontsize=14)\n",
    "axes[3].set_ylabel('Number of Images', fontsize=14)\n",
    "axes[3].tick_params(axis='x', rotation=45)\n",
    "axes[3].legend()\n",
    "axes[3].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81Am2tqunP7S"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# Mapping of art styles (classes) to sentiment categories (binary labels)\n",
    "class_to_main_class = {\n",
    "    \"fauvism\": [1, 0, 0, 0],               # Emotion Through Color\n",
    "    \"expressionism\": [1, 0, 0, 0],\n",
    "    \"abstract_expressionism\": [1, 0, 0, 0],\n",
    "\n",
    "    \"cubism\": [0, 1, 0, 0],                # Visual Complexity\n",
    "    \"surrealism\": [0, 1, 0, 0],\n",
    "    \"op_art\": [0, 1, 0, 0],\n",
    "\n",
    "    \"futurism\": [0, 0, 1, 0],              # Movement and Flow\n",
    "    \"baroque\": [0, 0, 1, 0],\n",
    "    \"art_nouveau\": [0, 0, 1, 0],\n",
    "\n",
    "    \"realism\": [0, 0, 0, 1],               # Facial Expressions and Human Emotion\n",
    "    \"romanticism\": [0, 0, 0, 1]\n",
    "}\n",
    "\n",
    "# Custom dataset class for sentiment model\n",
    "class CustomSentimentDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        super().__init__(root_dir, transform=transform)\n",
    "        self.class_to_main_class = class_to_main_class\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_idx = self.samples[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = torch.tensor(self.class_to_main_class[self.classes[class_idx]], dtype=torch.float)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Path to dataset (modify to your local directory)\n",
    "dataset_path = \"/content/CSE499-MerakiNexus-AI_And_Defi/sentiment_model/sentiment_dataset\"\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load datasets\n",
    "train_data = CustomSentimentDataset(root_dir=f\"{dataset_path}/train\", transform=transform)\n",
    "val_data = CustomSentimentDataset(root_dir=f\"{dataset_path}/val\", transform=transform)\n",
    "test_data = CustomSentimentDataset(root_dir=f\"{dataset_path}/test\", transform=transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dsaEaRMAoAi8",
    "outputId": "d8c4d162-ad44-4926-8092-e35f1c542ed6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define sentiment categories\n",
    "sentiment_categories = [\"Emotion Through Color\", \"Visual Complexity\", \"Movement and Flow\", \"Facial Expressions and Human Emotion\"]\n",
    "\n",
    "# Define model based on ResNet50 with custom heads for the 4 sentiment classes\n",
    "class SentimentModelWithResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(SentimentModelWithResNet50, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "        # Get the in_features from ResNet50's final fully connected layer\n",
    "        in_features = self.resnet.fc.in_features\n",
    "\n",
    "        # Remove the final fully connected layer (so that we can add custom heads)\n",
    "        self.resnet.fc = nn.Identity()\n",
    "\n",
    "        # Custom heads for the 4 sentiment classes\n",
    "        self.emotion_color_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)  # Output for Emotion Through Color\n",
    "        )\n",
    "\n",
    "        self.visual_complexity_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)  # Output for Visual Complexity\n",
    "        )\n",
    "\n",
    "        self.movement_flow_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)  # Output for Movement and Flow\n",
    "        )\n",
    "\n",
    "        self.human_emotion_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)  # Output for Facial Expressions and Human Emotion\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.resnet(x)\n",
    "        emotion_color = self.emotion_color_head(features)\n",
    "        visual_complexity = self.visual_complexity_head(features)\n",
    "        movement_flow = self.movement_flow_head(features)\n",
    "        human_emotion = self.human_emotion_head(features)\n",
    "        return emotion_color, visual_complexity, movement_flow, human_emotion\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "resNet50_model = SentimentModelWithResNet50(num_classes=4).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(resNet50_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training setup\n",
    "num_epochs = 25\n",
    "start_time = time.time()\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    resNet50_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\", unit=\"batch\") as tepoch:\n",
    "        for images, labels in tepoch:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            emotion_color, visual_complexity, movement_flow, human_emotion = resNet50_model(images)\n",
    "\n",
    "            loss1 = criterion(emotion_color.squeeze(), labels[:, 0])\n",
    "            loss2 = criterion(visual_complexity.squeeze(), labels[:, 1])\n",
    "            loss3 = criterion(movement_flow.squeeze(), labels[:, 2])\n",
    "            loss4 = criterion(human_emotion.squeeze(), labels[:, 3])\n",
    "\n",
    "            total_loss = loss1 + loss2 + loss3 + loss4\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += total_loss.item()\n",
    "\n",
    "            predictions = torch.cat([\n",
    "                torch.sigmoid(emotion_color),\n",
    "                torch.sigmoid(visual_complexity),\n",
    "                torch.sigmoid(movement_flow),\n",
    "                torch.sigmoid(human_emotion)\n",
    "            ], dim=1)\n",
    "\n",
    "            predicted_labels = (predictions > 0.5).float()\n",
    "            correct_train += (predicted_labels == labels).sum().item()\n",
    "            total_train += labels.size(0) * labels.size(1)\n",
    "\n",
    "            tepoch.set_postfix(loss=total_loss.item())\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed in {epoch_duration:.2f} seconds.\")\n",
    "    print(f\"Epoch Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "    train_accuracy = correct_train / total_train if total_train > 0 else 0\n",
    "    print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "    # Validation phase\n",
    "    resNet50_model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Validation\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            emotion_color, visual_complexity, movement_flow, human_emotion = resNet50_model(images)\n",
    "\n",
    "            pred1 = torch.sigmoid(emotion_color)\n",
    "            pred2 = torch.sigmoid(visual_complexity)\n",
    "            pred3 = torch.sigmoid(movement_flow)\n",
    "            pred4 = torch.sigmoid(human_emotion)\n",
    "\n",
    "            correct_val += (pred1 > 0.5).float().eq(labels[:, 0].unsqueeze(1)).sum().item()\n",
    "            correct_val += (pred2 > 0.5).float().eq(labels[:, 1].unsqueeze(1)).sum().item()\n",
    "            correct_val += (pred3 > 0.5).float().eq(labels[:, 2].unsqueeze(1)).sum().item()\n",
    "            correct_val += (pred4 > 0.5).float().eq(labels[:, 3].unsqueeze(1)).sum().item()\n",
    "            total_val += labels.size(0) * 4\n",
    "\n",
    "    val_accuracy = correct_val / total_val\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "# Total training time\n",
    "total_training_time = time.time() - start_time\n",
    "print(f\"\\nTotal Training Time: {total_training_time:.2f} seconds ({(total_training_time / 60):.2f} minutes)\")\n",
    "\n",
    "# Plotting Loss and Accuracy\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', color='blue')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', color='green')\n",
    "plt.title('Validation Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "b3KghdcG0fAQ",
    "outputId": "3b6775b2-2aed-42cd-f008-4f4deef16e94"
   },
   "outputs": [],
   "source": [
    "# Define sentiment categories\n",
    "sentiment_categories = [\"Emotion Through Color\", \"Visual Complexity\", \"Movement and Flow\", \"Facial Expressions and Human Emotion\"]\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "resNet50_model.eval()\n",
    "\n",
    "# Initialize variables to hold predictions and ground truths\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate over the test dataset\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        emotion_color, visual_complexity, movement_flow, human_emotion = resNet50_model(images)\n",
    "\n",
    "        # Convert logits to probabilities using sigmoid\n",
    "        pred_emotion_color = torch.sigmoid(emotion_color)\n",
    "        pred_visual_complexity = torch.sigmoid(visual_complexity)\n",
    "        pred_movement_flow = torch.sigmoid(movement_flow)\n",
    "        pred_human_emotion = torch.sigmoid(human_emotion)\n",
    "\n",
    "        # Binarize predictions (thresholding at 0.5)\n",
    "        bin_emotion_color = (pred_emotion_color > 0.5).float()\n",
    "        bin_visual_complexity = (pred_visual_complexity > 0.5).float()\n",
    "        bin_movement_flow = (pred_movement_flow > 0.5).float()\n",
    "        bin_human_emotion = (pred_human_emotion > 0.5).float()\n",
    "\n",
    "        # Concatenate predictions for multi-label output\n",
    "        batch_preds = torch.cat([bin_emotion_color, bin_visual_complexity, bin_movement_flow, bin_human_emotion], dim=1)\n",
    "        all_predictions.append(batch_preds)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "all_predictions = torch.cat(all_predictions, dim=0).cpu().numpy()\n",
    "all_labels = torch.cat(all_labels, dim=0).cpu().numpy()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (all_predictions == all_labels).sum() / all_labels.size\n",
    "\n",
    "# Compute evaluation metrics\n",
    "precision = precision_score(all_labels, all_predictions, average='samples')\n",
    "recall = recall_score(all_labels, all_predictions, average='samples')\n",
    "f1 = f1_score(all_labels, all_predictions, average='samples')\n",
    "\n",
    "# Multi-label confusion matrices\n",
    "mcm = multilabel_confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Plot confusion matrices for each class\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    sns.heatmap(mcm[i], annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Pred Negative', 'Pred Positive'],\n",
    "                yticklabels=['True Negative', 'True Positive'],\n",
    "                ax=ax)\n",
    "    ax.set_title(f'Confusion Matrix for {sentiment_categories[i]}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1-Score: {f1 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XbHm7Frp07B5",
    "outputId": "beac4893-583c-41e8-8d37-e3bb4e8dc8c6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define sentiment categories\n",
    "sentiment_categories = [\"Emotion Through Color\", \"Visual Complexity\", \"Movement and Flow\", \"Facial Expressions and Human Emotion\"]\n",
    "\n",
    "# Define model based on ResNet101 with custom heads for the 4 sentiment classes\n",
    "class SentimentModelWithResNet101(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(SentimentModelWithResNet101, self).__init__()\n",
    "        self.resnet = models.resnet101(pretrained=True)\n",
    "\n",
    "        # Get the in_features from ResNet101's final fully connected layer\n",
    "        in_features = self.resnet.fc.in_features\n",
    "\n",
    "        # Remove the final fully connected layer (so that we can add custom heads)\n",
    "        self.resnet.fc = nn.Identity()\n",
    "\n",
    "        # Custom heads for the 4 sentiment classes\n",
    "        self.emotion_color_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)  # Output for Emotion Through Color\n",
    "        )\n",
    "\n",
    "        self.visual_complexity_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)  # Output for Visual Complexity\n",
    "        )\n",
    "\n",
    "        self.movement_flow_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)  # Output for Movement and Flow\n",
    "        )\n",
    "\n",
    "        self.human_emotion_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)  # Output for Facial Expressions and Human Emotion\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.resnet(x)\n",
    "        emotion_color = self.emotion_color_head(features)\n",
    "        visual_complexity = self.visual_complexity_head(features)\n",
    "        movement_flow = self.movement_flow_head(features)\n",
    "        human_emotion = self.human_emotion_head(features)\n",
    "        return emotion_color, visual_complexity, movement_flow, human_emotion\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "resNet101_model = SentimentModelWithResNet101(num_classes=4).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(resNet101_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training setup\n",
    "num_epochs = 25\n",
    "start_time = time.time()\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    resNet101_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\", unit=\"batch\") as tepoch:\n",
    "        for images, labels in tepoch:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            emotion_color, visual_complexity, movement_flow, human_emotion = resNet101_model(images)\n",
    "\n",
    "            loss1 = criterion(emotion_color.squeeze(), labels[:, 0])\n",
    "            loss2 = criterion(visual_complexity.squeeze(), labels[:, 1])\n",
    "            loss3 = criterion(movement_flow.squeeze(), labels[:, 2])\n",
    "            loss4 = criterion(human_emotion.squeeze(), labels[:, 3])\n",
    "\n",
    "            total_loss = loss1 + loss2 + loss3 + loss4\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += total_loss.item()\n",
    "\n",
    "            predictions = torch.cat([\n",
    "                torch.sigmoid(emotion_color),\n",
    "                torch.sigmoid(visual_complexity),\n",
    "                torch.sigmoid(movement_flow),\n",
    "                torch.sigmoid(human_emotion)\n",
    "            ], dim=1)\n",
    "\n",
    "            predicted_labels = (predictions > 0.5).float()\n",
    "            correct_train += (predicted_labels == labels).sum().item()\n",
    "            total_train += labels.size(0) * labels.size(1)\n",
    "\n",
    "            tepoch.set_postfix(loss=total_loss.item())\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed in {epoch_duration:.2f} seconds.\")\n",
    "    print(f\"Epoch Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "    train_accuracy = correct_train / total_train if total_train > 0 else 0\n",
    "    print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "    # Validation phase\n",
    "    resNet101_model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Validation\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            emotion_color, visual_complexity, movement_flow, human_emotion = resNet101_model(images)\n",
    "\n",
    "            pred1 = torch.sigmoid(emotion_color)\n",
    "            pred2 = torch.sigmoid(visual_complexity)\n",
    "            pred3 = torch.sigmoid(movement_flow)\n",
    "            pred4 = torch.sigmoid(human_emotion)\n",
    "\n",
    "            correct_val += (pred1 > 0.5).float().eq(labels[:, 0].unsqueeze(1)).sum().item()\n",
    "            correct_val += (pred2 > 0.5).float().eq(labels[:, 1].unsqueeze(1)).sum().item()\n",
    "            correct_val += (pred3 > 0.5).float().eq(labels[:, 2].unsqueeze(1)).sum().item()\n",
    "            correct_val += (pred4 > 0.5).float().eq(labels[:, 3].unsqueeze(1)).sum().item()\n",
    "            total_val += labels.size(0) * 4\n",
    "\n",
    "    val_accuracy = correct_val / total_val\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "# Total training time\n",
    "total_training_time = time.time() - start_time\n",
    "print(f\"\\nTotal Training Time: {total_training_time:.2f} seconds ({(total_training_time / 60):.2f} minutes)\")\n",
    "\n",
    "# Plotting Loss and Accuracy\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', color='blue')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', color='green')\n",
    "plt.title('Validation Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xPFbgYvR-Ur0",
    "outputId": "89b29862-4193-446d-fd59-8520a4f492e2"
   },
   "outputs": [],
   "source": [
    "# Define sentiment categories\n",
    "sentiment_categories = [\"Emotion Through Color\", \"Visual Complexity\", \"Movement and Flow\", \"Facial Expressions and Human Emotion\"]\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "resNet101_model.eval()\n",
    "\n",
    "# Initialize variables to hold predictions and ground truths\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate over the test dataset\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        emotion_color, visual_complexity, movement_flow, human_emotion = resNet101_model(images)\n",
    "\n",
    "        # Convert logits to probabilities using sigmoid\n",
    "        pred_emotion_color = torch.sigmoid(emotion_color)\n",
    "        pred_visual_complexity = torch.sigmoid(visual_complexity)\n",
    "        pred_movement_flow = torch.sigmoid(movement_flow)\n",
    "        pred_human_emotion = torch.sigmoid(human_emotion)\n",
    "\n",
    "        # Binarize predictions (thresholding at 0.5)\n",
    "        bin_emotion_color = (pred_emotion_color > 0.5).float()\n",
    "        bin_visual_complexity = (pred_visual_complexity > 0.5).float()\n",
    "        bin_movement_flow = (pred_movement_flow > 0.5).float()\n",
    "        bin_human_emotion = (pred_human_emotion > 0.5).float()\n",
    "\n",
    "        # Concatenate predictions for multi-label output\n",
    "        batch_preds = torch.cat([bin_emotion_color, bin_visual_complexity, bin_movement_flow, bin_human_emotion], dim=1)\n",
    "        all_predictions.append(batch_preds)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "all_predictions = torch.cat(all_predictions, dim=0).cpu().numpy()\n",
    "all_labels = torch.cat(all_labels, dim=0).cpu().numpy()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (all_predictions == all_labels).sum() / all_labels.size\n",
    "\n",
    "# Compute evaluation metrics\n",
    "precision = precision_score(all_labels, all_predictions, average='samples')\n",
    "recall = recall_score(all_labels, all_predictions, average='samples')\n",
    "f1 = f1_score(all_labels, all_predictions, average='samples')\n",
    "\n",
    "# Multi-label confusion matrices\n",
    "mcm = multilabel_confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Plot confusion matrices for each class\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    sns.heatmap(mcm[i], annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Pred Negative', 'Pred Positive'],\n",
    "                yticklabels=['True Negative', 'True Positive'],\n",
    "                ax=ax)\n",
    "    ax.set_title(f'Confusion Matrix for {sentiment_categories[i]}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1-Score: {f1 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "8c33b50be52740b8bef46c411fda1b1f",
      "cc23677b96f446ae880069fc4d8565f7",
      "f3cc96f9b5f74f4eb98a063eae2aeb68",
      "88fe4dbc0dea4fd88f9dc0ccf930544a",
      "be3e360b3f124eee9d5d37b7039a0b44",
      "83389e1458f24ad8a2d98b2106232e48",
      "1ff5e6e28c50464b9937972bf436f022",
      "bea5fcdcd268456ca3cb8b99abd51de5",
      "8b5f9f001dc54a6a93e7acc3cfc9d055",
      "dd0057f7c1a94fdf83df43c9300724af",
      "62b2a6ede6ba4daaa39c684d5ef0dbc7"
     ]
    },
    "id": "sWiL7iZT-0S3",
    "outputId": "ea9a4ea3-d241-4c87-b6b1-2f84d26ba0cb"
   },
   "outputs": [],
   "source": [
    "!pip install timm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import timm\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define sentiment categories\n",
    "sentiment_categories = [\"Emotion Through Color\", \"Visual Complexity\", \"Movement and Flow\", \"Facial Expressions and Human Emotion\"]\n",
    "\n",
    "# Define ViT-based sentiment model with 4 custom heads\n",
    "class SentimentModelWithViT(nn.Module):\n",
    "    def __init__(self, backbone_name=\"vit_base_patch16_224\", num_classes=4):\n",
    "        super(SentimentModelWithViT, self).__init__()\n",
    "        self.vit = timm.create_model(backbone_name, pretrained=True)\n",
    "\n",
    "        # Extract the feature dimension from ViT head\n",
    "        in_features = self.vit.head.in_features\n",
    "        self.vit.head = nn.Identity()  # Remove the classification head\n",
    "\n",
    "        # Four sentiment heads\n",
    "        self.emotion_color_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.visual_complexity_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.movement_flow_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.human_emotion_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.vit(x)\n",
    "        return (\n",
    "            self.emotion_color_head(features),\n",
    "            self.visual_complexity_head(features),\n",
    "            self.movement_flow_head(features),\n",
    "            self.human_emotion_head(features)\n",
    "        )\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vit_model = SentimentModelWithViT().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(vit_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training settings\n",
    "num_epochs = 25\n",
    "start_time = time.time()\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    vit_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\", unit=\"batch\") as tepoch:\n",
    "        for images, labels in tepoch:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            emotion_color, visual_complexity, movement_flow, human_emotion = vit_model(images)\n",
    "\n",
    "            # Loss calculation\n",
    "            loss1 = criterion(emotion_color.squeeze(), labels[:, 0])\n",
    "            loss2 = criterion(visual_complexity.squeeze(), labels[:, 1])\n",
    "            loss3 = criterion(movement_flow.squeeze(), labels[:, 2])\n",
    "            loss4 = criterion(human_emotion.squeeze(), labels[:, 3])\n",
    "            total_loss = loss1 + loss2 + loss3 + loss4\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += total_loss.item()\n",
    "\n",
    "            # Accuracy calculation\n",
    "            predictions = torch.cat([\n",
    "                torch.sigmoid(emotion_color),\n",
    "                torch.sigmoid(visual_complexity),\n",
    "                torch.sigmoid(movement_flow),\n",
    "                torch.sigmoid(human_emotion)\n",
    "            ], dim=1)\n",
    "\n",
    "            predicted_labels = (predictions > 0.5).float()\n",
    "            correct_train += (predicted_labels == labels).sum().item()\n",
    "            total_train += labels.size(0) * labels.size(1)\n",
    "\n",
    "            tepoch.set_postfix(loss=total_loss.item())\n",
    "\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    print(f\"Epoch {epoch+1} completed in {epoch_duration:.2f} seconds.\")\n",
    "    print(f\"Epoch Loss: {running_loss / len(train_loader)}\")\n",
    "    train_accuracy = correct_train / total_train if total_train > 0 else 0\n",
    "    print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "    # Validation\n",
    "    vit_model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Validation\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            emotion_color, visual_complexity, movement_flow, human_emotion = vit_model(images)\n",
    "\n",
    "            pred1 = torch.sigmoid(emotion_color)\n",
    "            pred2 = torch.sigmoid(visual_complexity)\n",
    "            pred3 = torch.sigmoid(movement_flow)\n",
    "            pred4 = torch.sigmoid(human_emotion)\n",
    "\n",
    "            correct_val += (pred1 > 0.5).float().eq(labels[:, 0].unsqueeze(1)).sum().item()\n",
    "            correct_val += (pred2 > 0.5).float().eq(labels[:, 1].unsqueeze(1)).sum().item()\n",
    "            correct_val += (pred3 > 0.5).float().eq(labels[:, 2].unsqueeze(1)).sum().item()\n",
    "            correct_val += (pred4 > 0.5).float().eq(labels[:, 3].unsqueeze(1)).sum().item()\n",
    "            total_val += labels.size(0) * 4\n",
    "\n",
    "    val_accuracy = correct_val / total_val\n",
    "    print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "# Training complete\n",
    "total_training_time = time.time() - start_time\n",
    "print(f\"\\nTotal Training Time: {total_training_time:.2f} seconds ({total_training_time / 60:.2f} minutes)\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', color='blue')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', color='green')\n",
    "plt.title('Validation Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JMv9Db1rPEyR",
    "outputId": "75e72f70-ed04-47b3-f6c2-7de2cc591068"
   },
   "outputs": [],
   "source": [
    "# Define sentiment categories\n",
    "sentiment_categories = [\"Emotion Through Color\", \"Visual Complexity\", \"Movement and Flow\", \"Facial Expressions and Human Emotion\"]\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "vit_model.eval()\n",
    "\n",
    "# Initialize variables to hold predictions and ground truths\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate over the test dataset\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        emotion_color, visual_complexity, movement_flow, human_emotion = vit_model(images)\n",
    "\n",
    "        # Convert logits to probabilities using sigmoid\n",
    "        pred_emotion_color = torch.sigmoid(emotion_color)\n",
    "        pred_visual_complexity = torch.sigmoid(visual_complexity)\n",
    "        pred_movement_flow = torch.sigmoid(movement_flow)\n",
    "        pred_human_emotion = torch.sigmoid(human_emotion)\n",
    "\n",
    "        # Binarize predictions (thresholding at 0.5)\n",
    "        bin_emotion_color = (pred_emotion_color > 0.5).float()\n",
    "        bin_visual_complexity = (pred_visual_complexity > 0.5).float()\n",
    "        bin_movement_flow = (pred_movement_flow > 0.5).float()\n",
    "        bin_human_emotion = (pred_human_emotion > 0.5).float()\n",
    "\n",
    "        # Concatenate predictions for multi-label output\n",
    "        batch_preds = torch.cat([bin_emotion_color, bin_visual_complexity, bin_movement_flow, bin_human_emotion], dim=1)\n",
    "        all_predictions.append(batch_preds)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "all_predictions = torch.cat(all_predictions, dim=0).cpu().numpy()\n",
    "all_labels = torch.cat(all_labels, dim=0).cpu().numpy()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (all_predictions == all_labels).sum() / all_labels.size\n",
    "\n",
    "# Compute evaluation metrics\n",
    "precision = precision_score(all_labels, all_predictions, average='samples')\n",
    "recall = recall_score(all_labels, all_predictions, average='samples')\n",
    "f1 = f1_score(all_labels, all_predictions, average='samples')\n",
    "\n",
    "# Multi-label confusion matrices\n",
    "mcm = multilabel_confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Plot confusion matrices for each class\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    sns.heatmap(mcm[i], annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Pred Negative', 'Pred Positive'],\n",
    "                yticklabels=['True Negative', 'True Positive'],\n",
    "                ax=ax)\n",
    "    ax.set_title(f'Confusion Matrix for {sentiment_categories[i]}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1-Score: {f1 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UQBV_pS5Pnae",
    "outputId": "fcee45e9-9c63-40bc-9140-5fed6e68c371"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import timm\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define sentiment categories\n",
    "sentiment_categories = [\"Emotion Through Color\", \"Visual Complexity\", \"Movement and Flow\", \"Facial Expressions and Human Emotion\"]\n",
    "\n",
    "# Define model based on EfficientNet with custom heads for the 4 sentiment classes\n",
    "class SentimentModelWithEfficientNet(nn.Module):\n",
    "    def __init__(self, backbone_name=\"efficientnet_b0\", num_classes=4):\n",
    "        super(SentimentModelWithEfficientNet, self).__init__()\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=True)\n",
    "        in_features = self.backbone.classifier.in_features  # Get input size of the head\n",
    "        self.backbone.classifier = nn.Identity()  # Remove the classification layer\n",
    "\n",
    "        # Four custom heads for each sentiment dimension\n",
    "        self.emotion_color_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.visual_complexity_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.movement_flow_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.human_emotion_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return (\n",
    "            self.emotion_color_head(features),\n",
    "            self.visual_complexity_head(features),\n",
    "            self.movement_flow_head(features),\n",
    "            self.human_emotion_head(features)\n",
    "        )\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "effnet_model = SentimentModelWithEfficientNet().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(effnet_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training setup\n",
    "num_epochs = 25\n",
    "start_time = time.time()\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    effnet_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\", unit=\"batch\") as tepoch:\n",
    "        for images, labels in tepoch:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ec, vc, mf, he = effnet_model(images)\n",
    "\n",
    "            loss1 = criterion(ec.squeeze(), labels[:, 0])\n",
    "            loss2 = criterion(vc.squeeze(), labels[:, 1])\n",
    "            loss3 = criterion(mf.squeeze(), labels[:, 2])\n",
    "            loss4 = criterion(he.squeeze(), labels[:, 3])\n",
    "            total_loss = loss1 + loss2 + loss3 + loss4\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += total_loss.item()\n",
    "\n",
    "            predictions = torch.cat([\n",
    "                torch.sigmoid(ec),\n",
    "                torch.sigmoid(vc),\n",
    "                torch.sigmoid(mf),\n",
    "                torch.sigmoid(he)\n",
    "            ], dim=1)\n",
    "\n",
    "            predicted_labels = (predictions > 0.5).float()\n",
    "            correct_train += (predicted_labels == labels).sum().item()\n",
    "            total_train += labels.size(0) * labels.size(1)\n",
    "\n",
    "            tepoch.set_postfix(loss=total_loss.item())\n",
    "\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    print(f\"Epoch {epoch+1} completed in {epoch_duration:.2f} seconds.\")\n",
    "    print(f\"Epoch Loss: {running_loss / len(train_loader)}\")\n",
    "    train_accuracy = correct_train / total_train\n",
    "    print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "    # Validation\n",
    "    effnet_model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Validation\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            ec, vc, mf, he = effnet_model(images)\n",
    "\n",
    "            pred1 = torch.sigmoid(ec)\n",
    "            pred2 = torch.sigmoid(vc)\n",
    "            pred3 = torch.sigmoid(mf)\n",
    "            pred4 = torch.sigmoid(he)\n",
    "\n",
    "            correct_val += (pred1 > 0.5).float().eq(labels[:, 0].unsqueeze(1)).sum().item()\n",
    "            correct_val += (pred2 > 0.5).float().eq(labels[:, 1].unsqueeze(1)).sum().item()\n",
    "            correct_val += (pred3 > 0.5).float().eq(labels[:, 2].unsqueeze(1)).sum().item()\n",
    "            correct_val += (pred4 > 0.5).float().eq(labels[:, 3].unsqueeze(1)).sum().item()\n",
    "            total_val += labels.size(0) * 4\n",
    "\n",
    "    val_accuracy = correct_val / total_val\n",
    "    print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "# Training complete\n",
    "total_training_time = time.time() - start_time\n",
    "print(f\"\\nTotal Training Time: {total_training_time:.2f} seconds ({total_training_time / 60:.2f} minutes)\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', color='blue')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', color='green')\n",
    "plt.title('Validation Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xjtlaXKJgb4_",
    "outputId": "6e3d77ed-3375-437b-bb1b-69fde0e0ca80"
   },
   "outputs": [],
   "source": [
    "# Define sentiment categories\n",
    "sentiment_categories = [\"Emotion Through Color\", \"Visual Complexity\", \"Movement and Flow\", \"Facial Expressions and Human Emotion\"]\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "effnet_model.eval()\n",
    "\n",
    "# Initialize variables to hold predictions and ground truths\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate over the test dataset\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        emotion_color, visual_complexity, movement_flow, human_emotion = effnet_model(images)\n",
    "\n",
    "        # Convert logits to probabilities using sigmoid\n",
    "        pred_emotion_color = torch.sigmoid(emotion_color)\n",
    "        pred_visual_complexity = torch.sigmoid(visual_complexity)\n",
    "        pred_movement_flow = torch.sigmoid(movement_flow)\n",
    "        pred_human_emotion = torch.sigmoid(human_emotion)\n",
    "\n",
    "        # Binarize predictions (thresholding at 0.5)\n",
    "        bin_emotion_color = (pred_emotion_color > 0.5).float()\n",
    "        bin_visual_complexity = (pred_visual_complexity > 0.5).float()\n",
    "        bin_movement_flow = (pred_movement_flow > 0.5).float()\n",
    "        bin_human_emotion = (pred_human_emotion > 0.5).float()\n",
    "\n",
    "        # Concatenate predictions for multi-label output\n",
    "        batch_preds = torch.cat([bin_emotion_color, bin_visual_complexity, bin_movement_flow, bin_human_emotion], dim=1)\n",
    "        all_predictions.append(batch_preds)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "all_predictions = torch.cat(all_predictions, dim=0).cpu().numpy()\n",
    "all_labels = torch.cat(all_labels, dim=0).cpu().numpy()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (all_predictions == all_labels).sum() / all_labels.size\n",
    "\n",
    "# Compute evaluation metrics\n",
    "precision = precision_score(all_labels, all_predictions, average='samples')\n",
    "recall = recall_score(all_labels, all_predictions, average='samples')\n",
    "f1 = f1_score(all_labels, all_predictions, average='samples')\n",
    "\n",
    "# Multi-label confusion matrices\n",
    "mcm = multilabel_confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Plot confusion matrices for each class\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    sns.heatmap(mcm[i], annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Pred Negative', 'Pred Positive'],\n",
    "                yticklabels=['True Negative', 'True Positive'],\n",
    "                ax=ax)\n",
    "    ax.set_title(f'Confusion Matrix for {sentiment_categories[i]}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1-Score: {f1 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CIRcUO3Kgjhk",
    "outputId": "1a031394-6392-4921-c46c-db827fdc5537"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define sentiment categories\n",
    "sentiment_categories = [\"Emotion Through Color\", \"Visual Complexity\", \"Movement and Flow\", \"Facial Expressions and Human Emotion\"]\n",
    "\n",
    "# Define AlexNet-based model with 4 custom heads\n",
    "class SentimentModelWithAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(SentimentModelWithAlexNet, self).__init__()\n",
    "        self.backbone = models.alexnet(pretrained=True)\n",
    "        in_features = self.backbone.classifier[6].in_features\n",
    "\n",
    "        # Remove the original classification head\n",
    "        self.backbone.classifier[6] = nn.Identity()\n",
    "\n",
    "        # Define 4 separate heads for each sentiment category\n",
    "        self.emotion_color_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.visual_complexity_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.movement_flow_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.human_emotion_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return (\n",
    "            self.emotion_color_head(features),\n",
    "            self.visual_complexity_head(features),\n",
    "            self.movement_flow_head(features),\n",
    "            self.human_emotion_head(features)\n",
    "        )\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "alexnet_model = SentimentModelWithAlexNet().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(alexnet_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training setup\n",
    "num_epochs = 25\n",
    "start_time = time.time()\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    alexnet_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\", unit=\"batch\") as tepoch:\n",
    "        for images, labels in tepoch:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ec, vc, mf, he = alexnet_model(images)\n",
    "\n",
    "            loss1 = criterion(ec.squeeze(), labels[:, 0])\n",
    "            loss2 = criterion(vc.squeeze(), labels[:, 1])\n",
    "            loss3 = criterion(mf.squeeze(), labels[:, 2])\n",
    "            loss4 = criterion(he.squeeze(), labels[:, 3])\n",
    "            total_loss = loss1 + loss2 + loss3 + loss4\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += total_loss.item()\n",
    "\n",
    "            predictions = torch.cat([\n",
    "                torch.sigmoid(ec),\n",
    "                torch.sigmoid(vc),\n",
    "                torch.sigmoid(mf),\n",
    "                torch.sigmoid(he)\n",
    "            ], dim=1)\n",
    "\n",
    "            predicted_labels = (predictions > 0.5).float()\n",
    "            correct_train += (predicted_labels == labels).sum().item()\n",
    "            total_train += labels.size(0) * labels.size(1)\n",
    "\n",
    "            tepoch.set_postfix(loss=total_loss.item())\n",
    "\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    print(f\"Epoch {epoch+1} completed in {epoch_duration:.2f} seconds.\")\n",
    "    print(f\"Epoch Loss: {running_loss / len(train_loader)}\")\n",
    "    train_accuracy = correct_train / total_train\n",
    "    print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "    # Validation loop\n",
    "    alexnet_model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Validation\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            ec, vc, mf, he = alexnet_model(images)\n",
    "\n",
    "            pred1 = torch.sigmoid(ec)\n",
    "            pred2 = torch.sigmoid(vc)\n",
    "            pred3 = torch.sigmoid(mf)\n",
    "            pred4 = torch.sigmoid(he)\n",
    "\n",
    "            correct_val += (pred1 > 0.5).float().eq(labels[:, 0].unsqueeze(1)).sum().item()\n",
    "            correct_val += (pred2 > 0.5).float().eq(labels[:, 1].unsqueeze(1)).sum().item()\n",
    "            correct_val += (pred3 > 0.5).float().eq(labels[:, 2].unsqueeze(1)).sum().item()\n",
    "            correct_val += (pred4 > 0.5).float().eq(labels[:, 3].unsqueeze(1)).sum().item()\n",
    "            total_val += labels.size(0) * 4\n",
    "\n",
    "    val_accuracy = correct_val / total_val\n",
    "    print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "# Final stats\n",
    "total_training_time = time.time() - start_time\n",
    "print(f\"\\nTotal Training Time: {total_training_time:.2f} seconds ({total_training_time / 60:.2f} minutes)\")\n",
    "\n",
    "# Plotting loss and accuracy\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', color='blue')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', color='green')\n",
    "plt.title('Validation Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Vtfx3vZwpzHN",
    "outputId": "f36f92d3-81c0-4fe8-af25-a4c4d396d13e"
   },
   "outputs": [],
   "source": [
    "# Define sentiment categories\n",
    "sentiment_categories = [\"Emotion Through Color\", \"Visual Complexity\", \"Movement and Flow\", \"Facial Expressions and Human Emotion\"]\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "alexnet_model.eval()\n",
    "\n",
    "# Initialize variables to hold predictions and ground truths\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate over the test dataset\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        emotion_color, visual_complexity, movement_flow, human_emotion = alexnet_model(images)\n",
    "\n",
    "        # Convert logits to probabilities using sigmoid\n",
    "        pred_emotion_color = torch.sigmoid(emotion_color)\n",
    "        pred_visual_complexity = torch.sigmoid(visual_complexity)\n",
    "        pred_movement_flow = torch.sigmoid(movement_flow)\n",
    "        pred_human_emotion = torch.sigmoid(human_emotion)\n",
    "\n",
    "        # Binarize predictions (thresholding at 0.5)\n",
    "        bin_emotion_color = (pred_emotion_color > 0.5).float()\n",
    "        bin_visual_complexity = (pred_visual_complexity > 0.5).float()\n",
    "        bin_movement_flow = (pred_movement_flow > 0.5).float()\n",
    "        bin_human_emotion = (pred_human_emotion > 0.5).float()\n",
    "\n",
    "        # Concatenate predictions for multi-label output\n",
    "        batch_preds = torch.cat([bin_emotion_color, bin_visual_complexity, bin_movement_flow, bin_human_emotion], dim=1)\n",
    "        all_predictions.append(batch_preds)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "all_predictions = torch.cat(all_predictions, dim=0).cpu().numpy()\n",
    "all_labels = torch.cat(all_labels, dim=0).cpu().numpy()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (all_predictions == all_labels).sum() / all_labels.size\n",
    "\n",
    "# Compute evaluation metrics\n",
    "precision = precision_score(all_labels, all_predictions, average='samples')\n",
    "recall = recall_score(all_labels, all_predictions, average='samples')\n",
    "f1 = f1_score(all_labels, all_predictions, average='samples')\n",
    "\n",
    "# Multi-label confusion matrices\n",
    "mcm = multilabel_confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Plot confusion matrices for each class\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    sns.heatmap(mcm[i], annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Pred Negative', 'Pred Positive'],\n",
    "                yticklabels=['True Negative', 'True Positive'],\n",
    "                ax=ax)\n",
    "    ax.set_title(f'Confusion Matrix for {sentiment_categories[i]}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1-Score: {f1 * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1ff5e6e28c50464b9937972bf436f022": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "62b2a6ede6ba4daaa39c684d5ef0dbc7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "83389e1458f24ad8a2d98b2106232e48": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88fe4dbc0dea4fd88f9dc0ccf930544a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd0057f7c1a94fdf83df43c9300724af",
      "placeholder": "​",
      "style": "IPY_MODEL_62b2a6ede6ba4daaa39c684d5ef0dbc7",
      "value": " 346M/346M [00:02&lt;00:00, 154MB/s]"
     }
    },
    "8b5f9f001dc54a6a93e7acc3cfc9d055": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8c33b50be52740b8bef46c411fda1b1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cc23677b96f446ae880069fc4d8565f7",
       "IPY_MODEL_f3cc96f9b5f74f4eb98a063eae2aeb68",
       "IPY_MODEL_88fe4dbc0dea4fd88f9dc0ccf930544a"
      ],
      "layout": "IPY_MODEL_be3e360b3f124eee9d5d37b7039a0b44"
     }
    },
    "be3e360b3f124eee9d5d37b7039a0b44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bea5fcdcd268456ca3cb8b99abd51de5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc23677b96f446ae880069fc4d8565f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83389e1458f24ad8a2d98b2106232e48",
      "placeholder": "​",
      "style": "IPY_MODEL_1ff5e6e28c50464b9937972bf436f022",
      "value": "model.safetensors: 100%"
     }
    },
    "dd0057f7c1a94fdf83df43c9300724af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3cc96f9b5f74f4eb98a063eae2aeb68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bea5fcdcd268456ca3cb8b99abd51de5",
      "max": 346284714,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b5f9f001dc54a6a93e7acc3cfc9d055",
      "value": 346284714
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
